{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Loader\n",
    "\n",
    "This notebook is used to load jobs in this repo to the `wandb/jobs` public project.\n",
    "- You will need to be logged into wandb and have access to the `wandb` entity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kill ports ahead of spinning up containers (you may need to restart docker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !lsof -i TCP:3307 | grep LISTEN | awk '{print $2}' | xargs kill -9\n",
    "# !lsof -i TCP:8000 | grep LISTEN | awk '{print $2}' | xargs kill -9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%env WANDB_API_KEY {wandb.api.api_key}\n",
    "%env WANDB_ENTITY megatruong\n",
    "%env WANDB_PROJECT jobz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python jobs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The SQL Query job depends on access to a database.  You can load this dummy database with the snippet below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1ba379b3d85e36c8d8896b472290d6c2c4bf6fa2b85b24235a977c81f227df2\n",
      "docker: Error response from daemon: driver failed programming external connectivity on endpoint quizzical_mccarthy (9b20ce06f6467848340a26d4da4ea653d7762223c862ac12ed95dd70663fca71): Bind for 0.0.0.0:3307 failed: port is already allocated.\n",
      "env: MYSQL_USER=sakila\n",
      "env: MYSQL_PASSWORD=p_ssW0rd\n"
     ]
    }
   ],
   "source": [
    "!docker run -p 3307:3306 -d sakiladb/mysql:latest\n",
    "%env MYSQL_USER sakila\n",
    "%env MYSQL_PASSWORD p_ssW0rd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run python jobs as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('jobs/openai_eval/job.py')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_jobs = list(Path('jobs').glob('**/*job.py'))\n",
    "python_jobs = python_jobs[:1]\n",
    "python_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NAME=openai_eval\n",
      "env: WANDB_JOBS_REPO_CONFIG=jobs/openai_eval/config.yml\n",
      "Obtaining file:///Users/andrewtruong/repos/launch-jobs/evals\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: mypy in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (1.1.1)\n",
      "Requirement already satisfied: langdetect in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (1.0.9)\n",
      "Requirement already satisfied: backoff in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (2.2.1)\n",
      "Requirement already satisfied: snowflake-connector-python[pandas] in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (3.0.2)\n",
      "Requirement already satisfied: openai>=0.27.2 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (0.27.2)\n",
      "Requirement already satisfied: tiktoken in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (0.3.3)\n",
      "Requirement already satisfied: nltk in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (3.7)\n",
      "Requirement already satisfied: pandas in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (1.5.2)\n",
      "Requirement already satisfied: tqdm in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (4.65.0)\n",
      "Requirement already satisfied: termcolor in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (2.2.0)\n",
      "Requirement already satisfied: filelock in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (3.9.0)\n",
      "Requirement already satisfied: mock in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (4.0.3)\n",
      "Requirement already satisfied: numpy in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (1.23.5)\n",
      "Requirement already satisfied: pydantic in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (1.10.7)\n",
      "Requirement already satisfied: matplotlib in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (3.6.2)\n",
      "Requirement already satisfied: sacrebleu in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (2.3.1)\n",
      "Requirement already satisfied: fire in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (0.5.0)\n",
      "Requirement already satisfied: pyzstd in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (0.15.6)\n",
      "Requirement already satisfied: pyyaml in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (6.0)\n",
      "Requirement already satisfied: lz4 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (3.1.3)\n",
      "Requirement already satisfied: blobfile in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from evals==0.1.1) (2.0.1)\n",
      "Requirement already satisfied: aiohttp in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from openai>=0.27.2->evals==0.1.1) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from openai>=0.27.2->evals==0.1.1) (2.28.1)\n",
      "Requirement already satisfied: lxml~=4.9 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from blobfile->evals==0.1.1) (4.9.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from blobfile->evals==0.1.1) (1.26.14)\n",
      "Requirement already satisfied: pycryptodomex~=3.8 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from blobfile->evals==0.1.1) (3.17)\n",
      "Requirement already satisfied: six in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from fire->evals==0.1.1) (1.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from matplotlib->evals==0.1.1) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from matplotlib->evals==0.1.1) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from matplotlib->evals==0.1.1) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from matplotlib->evals==0.1.1) (1.0.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from matplotlib->evals==0.1.1) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from matplotlib->evals==0.1.1) (22.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from matplotlib->evals==0.1.1) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from matplotlib->evals==0.1.1) (9.4.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from mypy->evals==0.1.1) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.10 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from mypy->evals==0.1.1) (4.4.0)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from mypy->evals==0.1.1) (1.0.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from nltk->evals==0.1.1) (2022.7.9)\n",
      "Requirement already satisfied: joblib in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from nltk->evals==0.1.1) (1.1.1)\n",
      "Requirement already satisfied: click in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from nltk->evals==0.1.1) (8.0.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from pandas->evals==0.1.1) (2022.7)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from sacrebleu->evals==0.1.1) (0.8.10)\n",
      "Requirement already satisfied: colorama in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from sacrebleu->evals==0.1.1) (0.4.6)\n",
      "Requirement already satisfied: portalocker in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from sacrebleu->evals==0.1.1) (2.7.0)\n",
      "Requirement already satisfied: pyOpenSSL<24.0.0,>=16.2.0 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from snowflake-connector-python[pandas]->evals==0.1.1) (23.0.0)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from snowflake-connector-python[pandas]->evals==0.1.1) (1.5.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from snowflake-connector-python[pandas]->evals==0.1.1) (2022.12.7)\n",
      "Requirement already satisfied: cryptography<41.0.0,>=3.1.0 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from snowflake-connector-python[pandas]->evals==0.1.1) (39.0.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from snowflake-connector-python[pandas]->evals==0.1.1) (1.15.1)\n",
      "Requirement already satisfied: oscrypto<2.0.0 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from snowflake-connector-python[pandas]->evals==0.1.1) (1.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from snowflake-connector-python[pandas]->evals==0.1.1) (3.4)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from snowflake-connector-python[pandas]->evals==0.1.1) (2.6.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from snowflake-connector-python[pandas]->evals==0.1.1) (2.0.4)\n",
      "Requirement already satisfied: pyarrow<10.1.0,>=10.0.1 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from snowflake-connector-python[pandas]->evals==0.1.1) (10.0.1)\n",
      "Requirement already satisfied: pycparser in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas]->evals==0.1.1) (2.21)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from aiohttp->openai>=0.27.2->evals==0.1.1) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from aiohttp->openai>=0.27.2->evals==0.1.1) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from aiohttp->openai>=0.27.2->evals==0.1.1) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from aiohttp->openai>=0.27.2->evals==0.1.1) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from aiohttp->openai>=0.27.2->evals==0.1.1) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/andrewtruong/opt/miniconda3/envs/pylaunch39/lib/python3.9/site-packages (from aiohttp->openai>=0.27.2->evals==0.1.1) (4.0.2)\n",
      "Building wheels for collected packages: evals\n",
      "  Building editable for evals (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for evals: filename=evals-0.1.1-0.editable-py3-none-any.whl size=4278 sha256=de17ee2073e505929ef6473d7ffdd440fe55b644b6e64eb30218eb8178aeb19f\n",
      "  Stored in directory: /private/var/folders/tz/w7nbszhj74s3s9lyvpv71mz80000gn/T/pip-ephem-wheel-cache-2_3j0aq_/wheels/c6/a9/b2/f4a8a1f184ea24764056d59bb9c6dc4b8b66e9c63e0668e034\n",
      "Successfully built evals\n",
      "Installing collected packages: evals\n",
      "  Attempting uninstall: evals\n",
      "    Found existing installation: evals 0.1.1\n",
      "    Uninstalling evals-0.1.1:\n",
      "      Successfully uninstalled evals-0.1.1\n",
      "Successfully installed evals-0.1.1\n",
      "fetch: 96 objects found, done.                                                  \n",
      "fetch: Fetching all references...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmegatruong\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "for job in python_jobs:\n",
    "    %env WANDB_NAME {job.parent.name}\n",
    "    %env WANDB_JOBS_REPO_CONFIG {job.parent/'config.yml'}\n",
    "    if (job.parent/'bootstrap.sh').is_file():\n",
    "        !bash {job.parent}/bootstrap.sh\n",
    "    !pip install -r {job.parent/'requirements.txt'}\n",
    "    !python {job}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker jobs\n",
    "- These jobs touch AWS, so they mount the `.aws` directory.\n",
    "- If you need to see the literal command, prepend `set -x &&` to the shell command"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Endpoints job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NAME=deploy_to_sagemaker_endpoints\n",
      "env: WANDB_JOBS_REPO_CONFIG=config_tensorflow.yml\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 301B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-buster       0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (3/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 301B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.9-buster       0.3s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (11/11) FINISHED                                              \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 301B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.9-buster       0.3s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 200B                                          0.0s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.9-buster@sha256:dda953d28cdcf52  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/6] WORKDIR /launch                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/6] COPY requirements.txt ./                                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/6] RUN pip install -r requirements.txt                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/6] COPY deploy_to_sagemaker_endpoints.py inference.py ./     0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [6/6] COPY config_pytorch.yml config_tensorflow.yml ./          0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:2ee7188d9eb26f9bdc57058cc29950b824d6e3a8f726c  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/deploy_to_sagemaker_endpoints           0.0s\n",
      "\u001b[0m\u001b[?25hwandb: Currently logged in as: megatruong (wandb). Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.14.0\n",
      "wandb: Run data is saved locally in /launch/wandb/run-20230320_155851-hwvafxsf\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run deploy_to_sagemaker_endpoints\n",
      "wandb: â­ï¸ View project at https://wandb.ai/wandb/jobs\n",
      "wandb: ðŸš€ View run at https://wandb.ai/wandb/jobs/runs/hwvafxsf\n",
      "wandb: sagemaker job: Downloading artifact from wandb\n",
      "wandb: Downloading large artifact model-sage-feather-1:v2, 273.69MB. 4 files... \n",
      "wandb:   4 of 4 files downloaded.  \n",
      "Done. 0:0:5.7\n",
      "wandb: sagemaker job: Creating temp directory for sagemaker model\n",
      "wandb: sagemaker job: Uploading model to S3\n",
      "wandb: sagemaker job: Deploy model to Sagemaker Endpoints (this may take a while...)\n",
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "---!The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "wandb: sagemaker job: Successfully deployed endpoint: tensorflow-inference-2023-03-20-15-59-14-510\n",
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: | 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\n",
      "wandb: Run summary:\n",
      "wandb: sagemaker_endpoint tensorflow-inference...\n",
      "wandb: \n",
      "wandb: ðŸš€ View run deploy_to_sagemaker_endpoints at: https://wandb.ai/wandb/jobs/runs/hwvafxsf\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230320_155851-hwvafxsf/logs\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_NAME deploy_to_sagemaker_endpoints\n",
    "%env WANDB_JOBS_REPO_CONFIG config_tensorflow.yml\n",
    "\n",
    "!sudo docker build -t $WANDB_NAME jobs/deploy_to_sagemaker_endpoints && \\\n",
    "sudo docker run \\\n",
    "   -v $HOME/.aws:/root/.aws:ro \\\n",
    "   -e WANDB_API_KEY=$WANDB_API_KEY \\\n",
    "   -e WANDB_ENTITY=$WANDB_ENTITY \\\n",
    "   -e WANDB_PROJECT=$WANDB_PROJECT \\\n",
    "   -e WANDB_NAME=$WANDB_NAME \\\n",
    "   -e WANDB_RUN_GROUP=$WANDB_RUN_GROUP \\\n",
    "   -e WANDB_JOBS_REPO_CONFIG=$WANDB_JOBS_REPO_CONFIG \\\n",
    "   $WANDB_NAME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nvidia Triton Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This job requires a running Triton Server.  You can start one with this snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LD_PRELOAD=\"/usr/lib/aarch64-linux-gnu/libgomp.so.1\"\n",
      "ERROR: ld.so: object '\"/usr/lib/aarch64-linux-gnu/libgomp.so.1\"' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/2)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (3/4)                                                         \n",
      "\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for nvcr.io/nvidia/tritonserver:22.11-py3     0.1s\n",
      "\u001b[34m => [auth] nvidia/tritonserver:pull,push token for nvcr.io                 0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (3/4)                                                         \n",
      "\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for nvcr.io/nvidia/tritonserver:22.11-py3     0.3s\n",
      "\u001b[34m => [auth] nvidia/tritonserver:pull,push token for nvcr.io                 0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (3/4)                                                         \n",
      "\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for nvcr.io/nvidia/tritonserver:22.11-py3     0.4s\n",
      "\u001b[34m => [auth] nvidia/tritonserver:pull,push token for nvcr.io                 0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (3/4)                                                         \n",
      "\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for nvcr.io/nvidia/tritonserver:22.11-py3     0.6s\n",
      "\u001b[34m => [auth] nvidia/tritonserver:pull,push token for nvcr.io                 0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.7s (6/6) FINISHED                                                \n",
      "\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for nvcr.io/nvidia/tritonserver:22.11-py3     0.6s\n",
      "\u001b[0m\u001b[34m => [auth] nvidia/tritonserver:pull,push token for nvcr.io                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [1/1] FROM nvcr.io/nvidia/tritonserver:22.11-py3@sha256:1cb912  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:31b28b6e5433f5bd8e8af980e8bbe7cf4dbc3a75b8325  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/tritonserver-wandb                      0.0s\n",
      "\u001b[0m\u001b[?25hWARNING: Published ports are discarded when using host network mode\n",
      "f420b8e228d36cadf96f9c214f4d77e2e8947b3f79fd7c9254103bad6a981067\n"
     ]
    }
   ],
   "source": [
    "# you may need this export on M1\n",
    "# related: https://github.com/keras-team/keras-tuner/issues/317#issuecomment-640181692\n",
    "%env LD_PRELOAD=\"/usr/lib/aarch64-linux-gnu/libgomp.so.1\"\n",
    "\n",
    "!sudo docker build -t tritonserver-wandb jobs/deploy_to_nvidia_triton/server && \\\n",
    "sudo docker run \\\n",
    "  -v $HOME/.aws:/root/.aws:ro \\\n",
    "  -p 8000:8000 \\\n",
    "  --rm --net=host -d \\\n",
    "  tritonserver-wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then launch the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NAME=deploy_to_nvidia_triton\n",
      "env: WANDB_JOBS_REPO_CONFIG=config_tensorflow.yml\n",
      "ERROR: ld.so: object '\"/usr/lib/aarch64-linux-gnu/libgomp.so.1\"' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 415B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-buster       0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 415B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-buster       0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 415B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-buster       0.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (11/11) FINISHED                                              \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 415B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.9-buster       0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 163B                                          0.0s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.9-buster@sha256:dda953d28cdcf52  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/6] WORKDIR /launch                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/6] COPY requirements.txt ./                                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/6] RUN pip install -r requirements.txt                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/6] COPY deploy_to_nvidia_triton.py ./                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [6/6] COPY config_pytorch.yml config_tensorflow.yml ./          0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:99252a22ceb99370ef67c18ea383f6e057f746af18655  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/deploy_to_nvidia_triton                 0.0s\n",
      "\u001b[0m\u001b[?25hwandb: Currently logged in as: megatruong (wandb). Use `wandb login --relogin` to force relogin\n",
      "wandb: ERROR wandb version 0.13.8.dev1 has been retired!  Please upgrade.\n",
      "wandb: wandb version 0.14.0 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.13.8.dev1\n",
      "wandb: Run data is saved locally in /launch/wandb/run-20230320_160132-6cvjiyyz\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run deploy_to_nvidia_triton\n",
      "wandb: â­ï¸ View project at https://wandb.ai/wandb/jobs\n",
      "wandb: ðŸš€ View run at https://wandb.ai/wandb/jobs/runs/6cvjiyyz\n",
      "wandb: triton job: Downloading wandb artifact\n",
      "wandb: Downloading large artifact model-sage-feather-1:v3, 273.69MB. 4 files... \n",
      "wandb:   4 of 4 files downloaded.  \n",
      "Done. 0:0:5.9\n",
      "wandb: triton job: Uploading model to Triton model repo (this may take a while...)\n",
      "wandb: Uploading keras_metadata.pb to models/model-sage-feather-1/3/model.savedmodel/keras_metadata.pb\n",
      "wandb: Uploading saved_model.pb to models/model-sage-feather-1/3/model.savedmodel/saved_model.pb\n",
      "wandb: Uploading variables/variables.data-00000-of-00001 to models/model-sage-feather-1/3/model.savedmodel/variables/variables.data-00000-of-00001\n",
      "wandb: Uploading variables/variables.index to models/model-sage-feather-1/3/model.savedmodel/variables/variables.index\n",
      "wandb: triton job: Loading model into Triton\n",
      "wandb: WARNING Did not find config.pbtxt for model-sage-feather-1/3.  Trying to autogenerate config...\n",
      "wandb: Using autogenerated config for model-sage-feather-1/3\n",
      "wandb: Generated config at: overloaded_config.pbtxt\n",
      "wandb: triton job: Finished deploying to Triton\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: Synced deploy_to_nvidia_triton: https://wandb.ai/wandb/jobs/runs/6cvjiyyz\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230320_160132-6cvjiyyz/logs\n",
      "wandb: ERROR wandb version 0.13.8.dev1 has been retired!  Please upgrade.\n",
      "wandb: wandb version 0.14.0 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_NAME deploy_to_nvidia_triton\n",
    "%env WANDB_JOBS_REPO_CONFIG config_tensorflow.yml\n",
    "\n",
    "!sudo docker build -t $WANDB_NAME jobs/deploy_to_nvidia_triton/deployer && \\\n",
    "sudo docker run \\\n",
    "   -v $HOME/.aws:/root/.aws:ro \\\n",
    "   -e WANDB_API_KEY=$WANDB_API_KEY \\\n",
    "   -e WANDB_ENTITY=$WANDB_ENTITY \\\n",
    "   -e WANDB_PROJECT=$WANDB_PROJECT \\\n",
    "   -e WANDB_NAME=$WANDB_NAME \\\n",
    "   -e WANDB_RUN_GROUP=$WANDB_RUN_GROUP \\\n",
    "   -e WANDB_JOBS_REPO_CONFIG=$WANDB_JOBS_REPO_CONFIG \\\n",
    "   --rm --net=host \\\n",
    "   $WANDB_NAME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nvidia Tensor RT Conversion Job\n",
    "This job requires a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NAME=optimize_with_nvidia_tensorrt\n",
      "env: WANDB_JOBS_REPO_CONFIG=config.yml\n",
      "ERROR: ld.so: object '\"/usr/lib/aarch64-linux-gnu/libgomp.so.1\"' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for nvcr.io/nvidia/tensorflow:22.12-tf2-py3   0.1s\n",
      "\u001b[34m => [auth] nvidia/tensorflow:pull,push token for nvcr.io                   0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for nvcr.io/nvidia/tensorflow:22.12-tf2-py3   0.3s\n",
      "\u001b[34m => [auth] nvidia/tensorflow:pull,push token for nvcr.io                   0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for nvcr.io/nvidia/tensorflow:22.12-tf2-py3   0.4s\n",
      "\u001b[34m => [auth] nvidia/tensorflow:pull,push token for nvcr.io                   0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for nvcr.io/nvidia/tensorflow:22.12-tf2-py3   0.6s\n",
      "\u001b[34m => [auth] nvidia/tensorflow:pull,push token for nvcr.io                   0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.7s (9/9) FINISHED                                                \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 203B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for nvcr.io/nvidia/tensorflow:22.12-tf2-py3   0.6s\n",
      "\u001b[0m\u001b[34m => [auth] nvidia/tensorflow:pull,push token for nvcr.io                   0.0s\n",
      "\u001b[0m\u001b[34m => [1/3] FROM nvcr.io/nvidia/tensorflow:22.12-tf2-py3@sha256:947e32a2649  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 77B                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/3] RUN pip install wandb                                     0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/3] COPY optimize_with_tensorrt.py config.yml ./              0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:0d1791b1e19e2faa42c5b2cc1592b82000dd39437e228  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/optimize_with_nvidia_tensorrt           0.0s\n",
      "\u001b[0m\u001b[?25h2023-03-20 16:02:15.893748: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "wandb: Currently logged in as: megatruong (wandb). Use `wandb login --relogin` to force relogin\n",
      "wandb: wandb version 0.14.0 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.13.11\n",
      "wandb: Run data is saved locally in /workspace/wandb/run-20230320_160220-gjsw7wly\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run optimize_with_nvidia_tensorrt\n",
      "wandb: â­ï¸ View project at https://wandb.ai/wandb/jobs\n",
      "wandb: ðŸš€ View run at https://wandb.ai/wandb/jobs/runs/gjsw7wly\n",
      "wandb: downloading model\n",
      "wandb: Downloading large artifact inceptionv3:latest, 96.53MB. 4 files... \n",
      "wandb:   4 of 4 files downloaded.  \n",
      "Done. 0:0:2.5\n",
      "2023-03-20 16:02:24.081101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.089393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.089662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.090210: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 16:02:24.090799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.091055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.091336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.247077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.247405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.247695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:24.247970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14790 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "wandb: converting model\n",
      "2023-03-20 16:02:45.836833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:45.837035: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2023-03-20 16:02:45.837225: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-20 16:02:45.837704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:45.837950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:45.838194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:45.838498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:45.838738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:45.838936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14790 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0\n",
      "2023-03-20 16:02:49.328835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:49.329061: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2023-03-20 16:02:49.329233: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-20 16:02:49.329691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:49.329940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:49.330160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:49.330431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:49.330646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 16:02:49.330811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14790 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0\n",
      "2023-03-20 16:02:50.485744: W tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:198] Calibration with FP32 or FP16 is not implemented. Falling back to use_calibration = False.Note that the default value of use_calibration is True.\n",
      "2023-03-20 16:02:50.746089: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:952] \n",
      "\n",
      "################################################################################\n",
      "TensorRT unsupported/non-converted OP Report:\n",
      "\t- NoOp -> 2x\n",
      "\t- Identity -> 1x\n",
      "\t- Placeholder -> 1x\n",
      "--------------------------------------------------------------------------------\n",
      "\t- Total nonconverted OPs: 4\n",
      "\t- Total nonconverted OP Types: 3\n",
      "For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops.\n",
      "################################################################################\n",
      "\n",
      "2023-03-20 16:02:50.822653: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:1280] The environment variable TF_TRT_MAX_ALLOWED_ENGINES=20 has no effect since there are only 1 TRT Engines with  at least minimum_segment_size=3 nodes.\n",
      "2023-03-20 16:02:50.829484: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:799] Number of TensorRT candidate segments: 1\n",
      "2023-03-20 16:02:51.098654: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:916] Replaced segment 0 consisting of 701 nodes by TRTEngineOp_000_000.\n",
      "wandb: saving converted model\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 94). These functions will not be directly callable after loading.\n",
      "wandb: Adding directory to artifact (./inceptionv3_trt_FP32)... Done. 0.6s\n",
      "wandb: benchmarking models\n",
      "wandb: Warming up...\n",
      "2023-03-20 16:03:02.280206: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
      "wandb: Benchmarking model...\n",
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "wandb: Warming up...\n",
      "wandb: Benchmarking model...\n",
      "wandb: done\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: - 187.131 MB of 187.131 MB uploaded (0.000 MB deduped)\n",
      "wandb: Run history:\n",
      "wandb:  after_opt_inference_time_ms â–‡â–ˆâ–†â–…â–…â–†â–‡â–†â–†â–†â–†â–†â–‡â–â–‡â–‡â–‡â–‡â–†â–‡â–†â–‡â–†â–ˆâ–‡â–ˆâ–ˆâ–†â–†â–†â–†â–‡â–ˆâ–†â–ˆâ–†â–ˆâ–‡â–…â–…\n",
      "wandb: before_opt_inference_time_ms â–ƒâ–…â–‚â–…â–„â–„â–ƒâ–ƒâ–…â–ƒâ–â–„â–„â–„â–…â–†â–„â–…â–ƒâ–„â–‚â–ƒâ–‚â–„â–…â–„â–ƒâ–…â–†â–„â–…â–†â–†â–†â–†â–†â–…â–…â–„â–ˆ\n",
      "wandb:            benchmarking_step â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:  after_opt_inference_time_ms 34.27005\n",
      "wandb: before_opt_inference_time_ms 146.43788\n",
      "wandb:            benchmarking_step 999\n",
      "wandb: \n",
      "wandb: ðŸš€ View run optimize_with_nvidia_tensorrt at: https://wandb.ai/wandb/jobs/runs/gjsw7wly\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230320_160220-gjsw7wly/logs\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_NAME optimize_with_nvidia_tensorrt\n",
    "%env WANDB_JOBS_REPO_CONFIG config.yml\n",
    "\n",
    "!sudo docker build -t $WANDB_NAME jobs/optimize_with_tensor_rt && \\\n",
    "sudo docker run \\\n",
    "    --gpus all \\\n",
    "    --runtime=nvidia \\\n",
    "    -e WANDB_API_KEY=$WANDB_API_KEY \\\n",
    "    -e WANDB_ENTITY=$WANDB_ENTITY \\\n",
    "    -e WANDB_PROJECT=$WANDB_PROJECT \\\n",
    "    -e WANDB_NAME=$WANDB_NAME \\\n",
    "    -e WANDB_RUN_GROUP=$WANDB_RUN_GROUP \\\n",
    "    -e WANDB_JOBS_REPO_CONFIG=$WANDB_JOBS_REPO_CONFIG \\\n",
    "    $WANDB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylaunch39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
