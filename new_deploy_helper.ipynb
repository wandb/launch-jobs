{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# model = tf.keras.applications.InceptionV3(\n",
    "#     include_top=True,\n",
    "#     weights=\"imagenet\",\n",
    "#     input_tensor=None,\n",
    "#     input_shape=None,\n",
    "#     pooling=None,\n",
    "#     classes=1000,\n",
    "#     classifier_activation=\"softmax\",\n",
    "# )\n",
    "\n",
    "# model.save('inceptionv3')\n",
    "\n",
    "# import wandb\n",
    "# with wandb.init(project='trt-testing') as run:\n",
    "#     art = wandb.Artifact('inceptionv3', 'inceptionv3')\n",
    "#     art.add_dir('inceptionv3')\n",
    "#     run.log_artifact(art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m OpenAI Evals are bugged\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m SQL Query Job cannot be run on M1 due to upstream connectorx issue.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m This job requires an Nvidia GPU.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m 🚀 Launching run into megatruong/example-launch-jobs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m 🚀 Launching run into megatruong/example-launch-jobs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m 🚀 Launching run into megatruong/example-launch-jobs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m 🚀 Launching run into megatruong/example-launch-jobs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m 🚀 Launching run into megatruong/example-launch-jobs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m 🚀 Launching run into megatruong/example-launch-jobs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927139287d2140009a3c14794fc965cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m Launching run in docker with command: docker run --rm -e WANDB_BASE_URL=https://api.wandb.ai -e WANDB_API_KEY -e WANDB_PROJECT=example-launch-jobs -e WANDB_ENTITY=megatruong -e WANDB_LAUNCH=True -e WANDB_RUN_ID=5knsvabf -e WANDB_DOCKER=wandb/job_deploy_to_sagemaker_endpoints:5a0b4abc7b178cf28279c7f7a725d8c801792988 -e WANDB_NAME='Deploy PyTorch Model' -e WANDB_CONFIG='{\"artifact\": \"wandb-artifact://megatruong/ptl-testing2/model-vgw632i7:v0\", \"framework\": \"pytorch\", \"framework_version\": \"1.12\", \"python_version\": \"py38\", \"sagemaker_role\": \"arn:aws:iam::687678353814:role/sagemaker\", \"sagemaker_bucket\": \"sagemaker-us-west-2-687678353814\", \"instance_type\": \"ml.c5.xlarge\", \"instance_count\": 1, \"sagemaker_model_setup_kwargs\": {}, \"sagemaker_model_deployment_kwargs\": {}}' -e WANDB_ARTIFACTS='{}' --net host --env-file /Users/andrewtruong/.wandb_launch/env.list --volume /Users/andrewtruong/.aws:/home/andrewtruong/.aws:ro --volume /Users/andrewtruong/.aws:/root/.aws:ro wandb/job_deploy_to_sagemaker_endpoints:5a0b4abc7b178cf28279c7f7a725d8c801792988\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m Launching run in docker with command: docker run --rm -e WANDB_BASE_URL=https://api.wandb.ai -e WANDB_API_KEY -e WANDB_PROJECT=example-launch-jobs -e WANDB_ENTITY=megatruong -e WANDB_LAUNCH=True -e WANDB_RUN_ID=oro7y897 -e WANDB_DOCKER=wandb/job_hello_world:5a0b4abc7b178cf28279c7f7a725d8c801792988 -e WANDB_NAME='Hello World Example' -e WANDB_CONFIG='{}' -e WANDB_ARTIFACTS='{}' --net host --env-file /Users/andrewtruong/.wandb_launch/env.list --volume /Users/andrewtruong/.aws:/home/andrewtruong/.aws:ro --volume /Users/andrewtruong/.aws:/root/.aws:ro wandb/job_hello_world:5a0b4abc7b178cf28279c7f7a725d8c801792988\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m Launching run in docker with command: docker run --rm -e WANDB_BASE_URL=https://api.wandb.ai -e WANDB_API_KEY -e WANDB_PROJECT=example-launch-jobs -e WANDB_ENTITY=megatruong -e WANDB_LAUNCH=True -e WANDB_RUN_ID=4yq91mhe -e WANDB_DOCKER=wandb/job_deploy_to_sagemaker_endpoints:5a0b4abc7b178cf28279c7f7a725d8c801792988 -e WANDB_NAME='Deploy TensorFlow Model' -e WANDB_CONFIG='{\"artifact\": \"wandb-artifact://megatruong/fashion-mnist-keras-triton/model-sage-feather-1:v2\", \"framework\": \"tensorflow\", \"framework_version\": \"2.10.0\", \"python_version\": \"py38\", \"sagemaker_role\": \"arn:aws:iam::687678353814:role/sagemaker\", \"sagemaker_bucket\": \"sagemaker-us-west-2-687678353814\", \"instance_type\": \"ml.c5.xlarge\", \"instance_count\": 1, \"sagemaker_model_setup_kwargs\": {}, \"sagemaker_model_deployment_kwargs\": {}}' -e WANDB_ARTIFACTS='{}' --net host --env-file /Users/andrewtruong/.wandb_launch/env.list --volume /Users/andrewtruong/.aws:/home/andrewtruong/.aws:ro --volume /Users/andrewtruong/.aws:/root/.aws:ro wandb/job_deploy_to_sagemaker_endpoints:5a0b4abc7b178cf28279c7f7a725d8c801792988\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m Launching run in docker with command: docker run --rm -e WANDB_BASE_URL=https://api.wandb.ai -e WANDB_API_KEY -e WANDB_PROJECT=example-launch-jobs -e WANDB_ENTITY=megatruong -e WANDB_LAUNCH=True -e WANDB_RUN_ID=mbfh6vek -e WANDB_DOCKER=wandb/job_http_webhook:5a0b4abc7b178cf28279c7f7a725d8c801792988 -e WANDB_NAME=Example -e WANDB_CONFIG='{\"repo\": \"wandb/launch-jobs\", \"ref\": \"main\", \"workflow\": \"generate-report.yml\", \"payload_inputs\": {\"template-file\": \"workflow_helpers/template.py\"}, \"github_api_token_env_var\": \"GITHUB_API_TOKEN\", \"retry_settings\": {\"attempts\": 3, \"backoff\": {\"multiplier\": 1, \"max\": 60}}}' -e WANDB_ARTIFACTS='{}' --net host --env-file /Users/andrewtruong/.wandb_launch/env.list --volume /Users/andrewtruong/.aws:/home/andrewtruong/.aws:ro --volume /Users/andrewtruong/.aws:/root/.aws:ro wandb/job_http_webhook:5a0b4abc7b178cf28279c7f7a725d8c801792988\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m Launching run in docker with command: docker run --rm -e WANDB_BASE_URL=https://api.wandb.ai -e WANDB_API_KEY -e WANDB_PROJECT=example-launch-jobs -e WANDB_ENTITY=megatruong -e WANDB_LAUNCH=True -e WANDB_RUN_ID=o8m74li3 -e WANDB_DOCKER=wandb/job_msft_teams_webhook:5a0b4abc7b178cf28279c7f7a725d8c801792988 -e WANDB_NAME=Example -e WANDB_CONFIG='{\"webhook_url\": \"https://sysadminwandb.webhook.office.com/webhookb2/1fc24ff9-fe1f-4cf2-ab3b-88d0189149fd@af722783-84b6-4adc-9c49-c792786eab4a/IncomingWebhook/11767fa7c40840e4bb52817bce8d59aa/44f0d0e4-adc1-4ca6-84bc-1768a8278b1f\", \"title\": \"New model deployed!\", \"artifact\": \"wandb-artifact://wandb/pytorch-lightning-e2e/nature-e1d5dg6m:latest\", \"alias\": \"${alias}\", \"text\": \"example text\", \"link_button\": {\"text\": \"Review Deployed Model\", \"url\": \"https://www.wandb.ai\"}, \"color\": \"#00FF00\", \"retry_settings\": {\"attempts\": 3, \"backoff\": {\"multiplier\": 1, \"max\": 60}}}' -e WANDB_ARTIFACTS='{}' --net host --env-file /Users/andrewtruong/.wandb_launch/env.list --volume /Users/andrewtruong/.aws:/home/andrewtruong/.aws:ro --volume /Users/andrewtruong/.aws:/root/.aws:ro wandb/job_msft_teams_webhook:5a0b4abc7b178cf28279c7f7a725d8c801792988\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m Launching run in docker with command: docker run --rm -e WANDB_BASE_URL=https://api.wandb.ai -e WANDB_API_KEY -e WANDB_PROJECT=example-launch-jobs -e WANDB_ENTITY=megatruong -e WANDB_LAUNCH=True -e WANDB_RUN_ID=pv15w3m9 -e WANDB_DOCKER=wandb/job_github_actions_workflow_dispatch:5a0b4abc7b178cf28279c7f7a725d8c801792988 -e WANDB_NAME='Generate Report Action' -e WANDB_CONFIG='{\"owner\": \"wandb\", \"repo\": \"launch-jobs\", \"ref\": \"main\", \"workflow\": \"generate-report.yml\", \"workflow_inputs\": {\"template-file\": \"workflow_helpers/template.py\"}, \"github_api_token_env_var\": \"GITHUB_API_TOKEN\", \"retry_settings\": {\"attempts\": 3, \"backoff\": {\"multiplier\": 1, \"max\": 60}}}' -e WANDB_ARTIFACTS='{}' --net host --env-file /Users/andrewtruong/.wandb_launch/env.list --volume /Users/andrewtruong/.aws:/home/andrewtruong/.aws:ro --volume /Users/andrewtruong/.aws:/root/.aws:ro wandb/job_github_actions_workflow_dispatch:5a0b4abc7b178cf28279c7f7a725d8c801792988\n",
      "wandb: Currently logged in as: megatruong. Use `wandb login --relogin` to force relogin\n",
      "wandb: Currently logged in as: megatruong. Use `wandb login --relogin` to force relogin\n",
      "wandb: Currently logged in as: megatruong. Use `wandb login --relogin` to force relogin\n",
      "wandb: Currently logged in as: megatruong. Use `wandb login --relogin` to force relogin\n",
      "wandb: Currently logged in as: megatruong. Use `wandb login --relogin` to force relogin\n",
      "wandb: Currently logged in as: megatruong. Use `wandb login --relogin` to force relogin\n",
      "wandb: wandb version 0.15.4 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.15.3\n",
      "wandb: Run data is saved locally in /launch/wandb/run-20230608_042408-pv15w3m9\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run Generate Report Action\n",
      "wandb: ⭐️ View project at https://wandb.ai/megatruong/example-launch-jobs\n",
      "wandb: 🚀 View run at https://wandb.ai/megatruong/example-launch-jobs/runs/pv15w3m9\n",
      "wandb: wandb version 0.15.4 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.15.3\n",
      "wandb: Run data is saved locally in /launch/wandb/run-20230608_042408-mbfh6vek\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run Example\n",
      "wandb: ⭐️ View project at https://wandb.ai/megatruong/example-launch-jobs\n",
      "wandb: 🚀 View run at https://wandb.ai/megatruong/example-launch-jobs/runs/mbfh6vek\n",
      "wandb: wandb version 0.15.4 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.15.3\n",
      "wandb: Run data is saved locally in /launch/wandb/run-20230608_042408-oro7y897\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run Hello World Example\n",
      "wandb: ⭐️ View project at https://wandb.ai/megatruong/example-launch-jobs\n",
      "wandb: 🚀 View run at https://wandb.ai/megatruong/example-launch-jobs/runs/oro7y897\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: wandb version 0.15.4 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.15.3\n",
      "wandb: Run data is saved locally in /launch/wandb/run-20230608_042408-o8m74li3\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run Example\n",
      "wandb: ⭐️ View project at https://wandb.ai/megatruong/example-launch-jobs\n",
      "wandb: 🚀 View run at https://wandb.ai/megatruong/example-launch-jobs/runs/o8m74li3\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: wandb version 0.15.4 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.15.3\n",
      "wandb: Run data is saved locally in /launch/wandb/run-20230608_042408-5knsvabf\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run Deploy PyTorch Model\n",
      "wandb: ⭐️ View project at https://wandb.ai/megatruong/example-launch-jobs\n",
      "wandb: 🚀 View run at https://wandb.ai/megatruong/example-launch-jobs/runs/5knsvabf\n",
      "wandb: sagemaker job: Downloading artifact from wandb\n",
      "wandb: wandb version 0.15.4 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.15.3\n",
      "wandb: Run data is saved locally in /launch/wandb/run-20230608_042408-4yq91mhe\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run Deploy TensorFlow Model\n",
      "wandb: ⭐️ View project at https://wandb.ai/megatruong/example-launch-jobs\n",
      "wandb: 🚀 View run at https://wandb.ai/megatruong/example-launch-jobs/runs/4yq91mhe\n",
      "wandb: sagemaker job: Downloading artifact from wandb\n",
      "wandb: Downloading large artifact model-vgw632i7:v0, 269.70MB. 1 files... \n",
      "wandb: Downloading large artifact model-sage-feather-1:v2, 273.69MB. 4 files... \n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb: hello world\n",
      "wandb: \n",
      "wandb: 🚀 View run Hello World Example at: https://wandb.ai/megatruong/example-launch-jobs/runs/oro7y897\n",
      "wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230608_042408-oro7y897/logs\n",
      "wandb: 🚀 View run Generate Report Action at: https://wandb.ai/megatruong/example-launch-jobs/runs/pv15w3m9\n",
      "wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230608_042408-pv15w3m9/logs\n",
      "wandb: 🚀 View run Example at: https://wandb.ai/megatruong/example-launch-jobs/runs/mbfh6vek\n",
      "wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230608_042408-mbfh6vek/logs\n",
      "wandb: 🚀 View run Example at: https://wandb.ai/megatruong/example-launch-jobs/runs/o8m74li3\n",
      "wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230608_042408-o8m74li3/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m 🚀 Launching run into megatruong/example-launch-jobs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m Launching run in docker with command: docker run --rm -e WANDB_BASE_URL=https://api.wandb.ai -e WANDB_API_KEY -e WANDB_PROJECT=example-launch-jobs -e WANDB_ENTITY=megatruong -e WANDB_LAUNCH=True -e WANDB_RUN_ID=gh9ccoys -e WANDB_DOCKER=wandb/job_deploy_to_nvidia_triton:5a0b4abc7b178cf28279c7f7a725d8c801792988 -e WANDB_NAME='Deploy PyTorch Model' -e WANDB_CONFIG='{\"artifact\": \"wandb-artifact://megatruong/ptl-testing2/my_model:v0\", \"framework\": \"pytorch\", \"triton_url\": \"localhost:8000\", \"triton_bucket\": \"andrew-triton-bucket\", \"triton_model_repo_path\": \"models\", \"triton_model_config_overrides\": {\"max_batch_size\": 32, \"input\": [{\"name\": \"conv1\", \"data_type\": \"TYPE_FP32\", \"dims\": [3, 28, 28]}], \"output\": [{\"name\": \"fc\", \"data_type\": \"TYPE_FP32\", \"dims\": [1]}]}}' -e WANDB_ARTIFACTS='{}' --net host --env-file /Users/andrewtruong/.wandb_launch/env.list --volume /Users/andrewtruong/.aws:/home/andrewtruong/.aws:ro --volume /Users/andrewtruong/.aws:/root/.aws:ro wandb/job_deploy_to_nvidia_triton:5a0b4abc7b178cf28279c7f7a725d8c801792988\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m 🚀 Launching run into megatruong/example-launch-jobs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m Launching run in docker with command: docker run --rm -e WANDB_BASE_URL=https://api.wandb.ai -e WANDB_API_KEY -e WANDB_PROJECT=example-launch-jobs -e WANDB_ENTITY=megatruong -e WANDB_LAUNCH=True -e WANDB_RUN_ID=jgj7pb8m -e WANDB_DOCKER=wandb/job_deploy_to_nvidia_triton:5a0b4abc7b178cf28279c7f7a725d8c801792988 -e WANDB_NAME='Deploy Ensemble Model' -e WANDB_CONFIG='{\"artifact\": \"wandb-artifact://megatruong/tritonserver-ensemble-testing10/ensemble_model:v0\", \"framework\": \"ensemble\", \"triton_url\": \"localhost:8000\", \"triton_bucket\": \"andrew-triton-bucket\", \"triton_model_repo_path\": \"models\", \"triton_model_config_overrides\": {\"version_policy\": {\"specific\": {\"versions\": [1]}}}}' -e WANDB_ARTIFACTS='{}' --net host --env-file /Users/andrewtruong/.wandb_launch/env.list --volume /Users/andrewtruong/.aws:/home/andrewtruong/.aws:ro --volume /Users/andrewtruong/.aws:/root/.aws:ro wandb/job_deploy_to_nvidia_triton:5a0b4abc7b178cf28279c7f7a725d8c801792988\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m 🚀 Launching run into megatruong/example-launch-jobs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[35mlaunch:\u001b[0m Launching run in docker with command: docker run --rm -e WANDB_BASE_URL=https://api.wandb.ai -e WANDB_API_KEY -e WANDB_PROJECT=example-launch-jobs -e WANDB_ENTITY=megatruong -e WANDB_LAUNCH=True -e WANDB_RUN_ID=i0cv77z8 -e WANDB_DOCKER=wandb/job_deploy_to_nvidia_triton:5a0b4abc7b178cf28279c7f7a725d8c801792988 -e WANDB_NAME='Deploy TensorFlow Model' -e WANDB_CONFIG='{\"artifact\": \"wandb-artifact://megatruong/fashion-mnist-keras-triton/model-sage-feather-1:v3\", \"framework\": \"tensorflow\", \"triton_url\": \"localhost:8000\", \"triton_bucket\": \"andrew-triton-bucket\", \"triton_model_repo_path\": \"models\", \"triton_model_config_overrides\": {}}' -e WANDB_ARTIFACTS='{}' --net host --env-file /Users/andrewtruong/.wandb_launch/env.list --volume /Users/andrewtruong/.aws:/home/andrewtruong/.aws:ro --volume /Users/andrewtruong/.aws:/root/.aws:ro wandb/job_deploy_to_nvidia_triton:5a0b4abc7b178cf28279c7f7a725d8c801792988\n",
      "wandb:   1 of 1 files downloaded.  \n",
      "Done. 0:0:6.9\n",
      "wandb: sagemaker job: Creating temp directory for sagemaker model\n",
      "wandb:   4 of 4 files downloaded.  \n",
      "Done. 0:0:7.3\n",
      "wandb: sagemaker job: Creating temp directory for sagemaker model\n",
      "wandb: Currently logged in as: megatruong. Use `wandb login --relogin` to force relogin\n",
      "wandb: Currently logged in as: megatruong. Use `wandb login --relogin` to force relogin\n",
      "wandb: Currently logged in as: megatruong. Use `wandb login --relogin` to force relogin\n",
      "wandb: wandb version 0.15.4 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.15.3\n",
      "wandb: Run data is saved locally in /launch/wandb/run-20230608_042418-gh9ccoys\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run Deploy PyTorch Model\n",
      "wandb: ⭐️ View project at https://wandb.ai/megatruong/example-launch-jobs\n",
      "wandb: 🚀 View run at https://wandb.ai/megatruong/example-launch-jobs/runs/gh9ccoys\n",
      "wandb: triton job: Downloading wandb artifact\n",
      "wandb: Downloading large artifact my_model:v0, 90.11MB. 1 files... \n",
      "wandb: wandb version 0.15.4 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.15.3\n",
      "wandb: Run data is saved locally in /launch/wandb/run-20230608_042418-i0cv77z8\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run Deploy TensorFlow Model\n",
      "wandb: ⭐️ View project at https://wandb.ai/megatruong/example-launch-jobs\n",
      "wandb: 🚀 View run at https://wandb.ai/megatruong/example-launch-jobs/runs/i0cv77z8\n",
      "wandb: triton job: Downloading wandb artifact\n",
      "wandb: wandb version 0.15.4 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.15.3\n",
      "wandb: Run data is saved locally in /launch/wandb/run-20230608_042418-jgj7pb8m\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run Deploy Ensemble Model\n",
      "wandb: ⭐️ View project at https://wandb.ai/megatruong/example-launch-jobs\n",
      "wandb: 🚀 View run at https://wandb.ai/megatruong/example-launch-jobs/runs/jgj7pb8m\n",
      "wandb: triton job: Downloading wandb artifact\n",
      "wandb: Downloading large artifact model-sage-feather-1:v3, 273.69MB. 4 files... \n",
      "wandb:   1 of 1 files downloaded.  \n",
      "Done. 0:0:3.2\n",
      "wandb: triton job: Uploading model to Triton model repo (this may take a while...)\n",
      "wandb: Uploading model.pt to models/my_model/0/model.pt\n",
      "wandb: triton job: Loading model into Triton\n",
      "wandb: Generated config at: overloaded_config.pbtxt\n",
      "wandb:   4 of 4 files downloaded.  ..\n",
      "Done. 0:0:8.1\n",
      "wandb: triton job: Uploading model to Triton model repo (this may take a while...)\n",
      "wandb: Uploading saved_model.pb to models/model-sage-feather-1/3/model.savedmodel/saved_model.pb\n",
      "wandb:   15 of 15 files downloaded.  \n",
      "wandb: triton job: Uploading model to Triton model repo (this may take a while...)\n",
      "wandb: Uploading ensemble/config.pbtxt to models/ensemble/config.pbtxt\n",
      "wandb: Uploading ensemble/1/ignoreme to models/ensemble/1/ignoreme\n",
      "wandb: Uploading recognition_postprocessing/config.pbtxt to models/recognition_postprocessing/config.pbtxt\n",
      "wandb: Uploading keras_metadata.pb to models/model-sage-feather-1/3/model.savedmodel/keras_metadata.pb\n",
      "wandb: Uploading recognition_postprocessing/1/model.py to models/recognition_postprocessing/1/model.py\n",
      "wandb: Uploading recognition_postprocessing/1/__pycache__/model.cpython-38.pyc to models/recognition_postprocessing/1/__pycache__/model.cpython-38.pyc\n",
      "wandb: Uploading variables/variables.data-00000-of-00001 to models/model-sage-feather-1/3/model.savedmodel/variables/variables.data-00000-of-00001\n",
      "wandb: Uploading detection_preprocessing/config.pbtxt to models/detection_preprocessing/config.pbtxt\n",
      "wandb: Uploading detection_preprocessing/1/model.py to models/detection_preprocessing/1/model.py\n",
      "wandb: Uploading detection_preprocessing/1/__pycache__/model.cpython-38.pyc to models/detection_preprocessing/1/__pycache__/model.cpython-38.pyc\n",
      "wandb: Uploading text_recognition/config.pbtxt to models/text_recognition/config.pbtxt\n",
      "wandb: Uploading text_recognition/1/model.onnx to models/text_recognition/1/model.onnx\n",
      "wandb: triton job: Finished deploying to Triton\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: Uploading variables/variables.index to models/model-sage-feather-1/3/model.savedmodel/variables/variables.index\n",
      "wandb: triton job: Loading model into Triton\n",
      "wandb: Uploading detection_postprocessing/config.pbtxt to models/detection_postprocessing/config.pbtxt\n",
      "wandb: Uploading detection_postprocessing/1/model.py to models/detection_postprocessing/1/model.py\n",
      "wandb: Uploading detection_postprocessing/1/__pycache__/model.cpython-38.pyc to models/detection_postprocessing/1/__pycache__/model.cpython-38.pyc\n",
      "wandb: Uploading text_detection/config.pbtxt to models/text_detection/config.pbtxt\n",
      "wandb: WARNING Did not find config.pbtxt for model-sage-feather-1/3.  Trying to autogenerate config...\n",
      "wandb: Uploading text_detection/1/model.onnx to models/text_detection/1/model.onnx\n",
      "wandb: sagemaker job: Uploading model to S3\n",
      "wandb: 🚀 View run Deploy PyTorch Model at: https://wandb.ai/megatruong/example-launch-jobs/runs/gh9ccoys\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230608_042418-gh9ccoys/logs\n",
      "wandb: triton job: Loading model into Triton\n",
      "wandb: sagemaker job: Deploy model to Sagemaker Endpoints (this may take a while...)\n",
      "wandb: Generated config at: overloaded_config.pbtxt\n",
      "wandb: sagemaker job: Uploading model to S3\n",
      "wandb: sagemaker job: Deploy model to Sagemaker Endpoints (this may take a while...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Unable to autogenerate config: timed out.  Continuing with empty base config.\n",
      "wandb: Generated config at: overloaded_config.pbtxt\n",
      "wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.\n",
      "wandb: \\ 0.006 MB of 0.009 MB uploaded (0.000 MB deduped)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: 🚀 View run Deploy Ensemble Model at: https://wandb.ai/megatruong/example-launch-jobs/runs/jgj7pb8m\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230608_042418-jgj7pb8m/logs\n",
      "Traceback (most recent call last):\n",
      "  File \"/launch/job.py\", line 145, in <module>\n",
      "    client.load_model(model_name, config=json.dumps(triton_configs))\n",
      "  File \"/usr/local/lib/python3.9/site-packages/tritonclient/http/_client.py\", line 652, in load_model\n",
      "    response = self._post(request_uri=request_uri,\n",
      "  File \"/usr/local/lib/python3.9/site-packages/tritonclient/http/_client.py\", line 284, in _post\n",
      "    response = self._client_stub.post(request_uri=request_uri,\n",
      "  File \"/usr/local/lib/python3.9/site-packages/geventhttpclient/client.py\", line 272, in post\n",
      "    return self.request(METHOD_POST, request_uri, body=body, headers=headers)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/geventhttpclient/client.py\", line 253, in request\n",
      "    response = HTTPSocketPoolResponse(sock, self._connection_pool,\n",
      "  File \"/usr/local/lib/python3.9/site-packages/geventhttpclient/response.py\", line 292, in __init__\n",
      "    super(HTTPSocketPoolResponse, self).__init__(sock, **kw)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/geventhttpclient/response.py\", line 164, in __init__\n",
      "    self._read_headers()\n",
      "  File \"/usr/local/lib/python3.9/site-packages/geventhttpclient/response.py\", line 184, in _read_headers\n",
      "    data = self._sock.recv(self.block_size)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/gevent/_socketcommon.py\", line 663, in recv\n",
      "    self._wait(self._read_event)\n",
      "  File \"src/gevent/_hub_primitives.py\", line 317, in gevent._gevent_c_hub_primitives.wait_on_socket\n",
      "  File \"src/gevent/_hub_primitives.py\", line 322, in gevent._gevent_c_hub_primitives.wait_on_socket\n",
      "  File \"src/gevent/_hub_primitives.py\", line 313, in gevent._gevent_c_hub_primitives._primitive_wait\n",
      "  File \"src/gevent/_hub_primitives.py\", line 314, in gevent._gevent_c_hub_primitives._primitive_wait\n",
      "  File \"src/gevent/_hub_primitives.py\", line 46, in gevent._gevent_c_hub_primitives.WaitOperationsGreenlet.wait\n",
      "  File \"src/gevent/_hub_primitives.py\", line 46, in gevent._gevent_c_hub_primitives.WaitOperationsGreenlet.wait\n",
      "  File \"src/gevent/_hub_primitives.py\", line 55, in gevent._gevent_c_hub_primitives.WaitOperationsGreenlet.wait\n",
      "  File \"src/gevent/_waiter.py\", line 154, in gevent._gevent_c_waiter.Waiter.get\n",
      "  File \"src/gevent/_greenlet_primitives.py\", line 61, in gevent._gevent_c_greenlet_primitives.SwitchOutGreenletWithLoop.switch\n",
      "  File \"src/gevent/_greenlet_primitives.py\", line 61, in gevent._gevent_c_greenlet_primitives.SwitchOutGreenletWithLoop.switch\n",
      "  File \"src/gevent/_greenlet_primitives.py\", line 65, in gevent._gevent_c_greenlet_primitives.SwitchOutGreenletWithLoop.switch\n",
      "  File \"src/gevent/_gevent_c_greenlet_primitives.pxd\", line 35, in gevent._gevent_c_greenlet_primitives._greenlet_switch\n",
      "socket.timeout: timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.\n",
      "wandb: 🚀 View run Deploy TensorFlow Model at: https://wandb.ai/megatruong/example-launch-jobs/runs/i0cv77z8\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230608_042418-i0cv77z8/logs\n",
      "Traceback (most recent call last):\n",
      "  File \"/launch/job.py\", line 145, in <module>\n",
      "    client.load_model(model_name, config=json.dumps(triton_configs))\n",
      "  File \"/usr/local/lib/python3.9/site-packages/tritonclient/http/_client.py\", line 652, in load_model\n",
      "    response = self._post(request_uri=request_uri,\n",
      "  File \"/usr/local/lib/python3.9/site-packages/tritonclient/http/_client.py\", line 284, in _post\n",
      "    response = self._client_stub.post(request_uri=request_uri,\n",
      "  File \"/usr/local/lib/python3.9/site-packages/geventhttpclient/client.py\", line 272, in post\n",
      "    return self.request(METHOD_POST, request_uri, body=body, headers=headers)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/geventhttpclient/client.py\", line 253, in request\n",
      "    response = HTTPSocketPoolResponse(sock, self._connection_pool,\n",
      "  File \"/usr/local/lib/python3.9/site-packages/geventhttpclient/response.py\", line 292, in __init__\n",
      "    super(HTTPSocketPoolResponse, self).__init__(sock, **kw)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/geventhttpclient/response.py\", line 164, in __init__\n",
      "    self._read_headers()\n",
      "  File \"/usr/local/lib/python3.9/site-packages/geventhttpclient/response.py\", line 184, in _read_headers\n",
      "    data = self._sock.recv(self.block_size)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/gevent/_socketcommon.py\", line 663, in recv\n",
      "    self._wait(self._read_event)\n",
      "  File \"src/gevent/_hub_primitives.py\", line 317, in gevent._gevent_c_hub_primitives.wait_on_socket\n",
      "  File \"src/gevent/_hub_primitives.py\", line 322, in gevent._gevent_c_hub_primitives.wait_on_socket\n",
      "  File \"src/gevent/_hub_primitives.py\", line 313, in gevent._gevent_c_hub_primitives._primitive_wait\n",
      "  File \"src/gevent/_hub_primitives.py\", line 314, in gevent._gevent_c_hub_primitives._primitive_wait\n",
      "  File \"src/gevent/_hub_primitives.py\", line 46, in gevent._gevent_c_hub_primitives.WaitOperationsGreenlet.wait\n",
      "  File \"src/gevent/_hub_primitives.py\", line 46, in gevent._gevent_c_hub_primitives.WaitOperationsGreenlet.wait\n",
      "  File \"src/gevent/_hub_primitives.py\", line 55, in gevent._gevent_c_hub_primitives.WaitOperationsGreenlet.wait\n",
      "  File \"src/gevent/_waiter.py\", line 154, in gevent._gevent_c_waiter.Waiter.get\n",
      "  File \"src/gevent/_greenlet_primitives.py\", line 61, in gevent._gevent_c_greenlet_primitives.SwitchOutGreenletWithLoop.switch\n",
      "  File \"src/gevent/_greenlet_primitives.py\", line 61, in gevent._gevent_c_greenlet_primitives.SwitchOutGreenletWithLoop.switch\n",
      "  File \"src/gevent/_greenlet_primitives.py\", line 65, in gevent._gevent_c_greenlet_primitives.SwitchOutGreenletWithLoop.switch\n",
      "  File \"src/gevent/_gevent_c_greenlet_primitives.pxd\", line 35, in gevent._gevent_c_greenlet_primitives._greenlet_switch\n",
      "socket.timeout: timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "wandb: sagemaker job: Successfully deployed endpoint: tensorflow-inference-2023-06-08-04-24-51-369\n",
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: | 0.004 MB of 0.007 MB uploaded (0.000 MB deduped)\n",
      "wandb: Run summary:\n",
      "wandb: sagemaker_endpoint tensorflow-inference...\n",
      "wandb: \n",
      "wandb: 🚀 View run Deploy TensorFlow Model at: https://wandb.ai/megatruong/example-launch-jobs/runs/4yq91mhe\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230608_042408-4yq91mhe/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "wandb: sagemaker job: Successfully deployed endpoint: pytorch-inference-2023-06-08-04-26-01-962\n",
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: \\ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "wandb: Run summary:\n",
      "wandb: sagemaker_endpoint pytorch-inference-20...\n",
      "wandb: \n",
      "wandb: 🚀 View run Deploy PyTorch Model at: https://wandb.ai/megatruong/example-launch-jobs/runs/5knsvabf\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230608_042408-5knsvabf/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Successfully deleted pytorch-inference-2023-06-08-04-26-01-962\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Successfully deleted tensorflow-inference-2023-06-08-04-24-51-369\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import wandb\n",
    "from wandb.sdk.internal.internal_api import Api as InternalApi\n",
    "from wandb.sdk.launch import launch, launch_add\n",
    "import re\n",
    "import boto3\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "import platform\n",
    "\n",
    "\n",
    "def image_name_to_artifact_name(s, alias=\"latest\"):\n",
    "    result = re.sub(r\"[:/]\", \"_\", s)\n",
    "    result = f\"job-{result}:{alias}\"\n",
    "    return result\n",
    "\n",
    "\n",
    "def _traverse_dict(d, path=None):\n",
    "    if path is None:\n",
    "        path = []\n",
    "    output = {}\n",
    "    for k, v in d.items():\n",
    "        new_path = path + [k]\n",
    "        if isinstance(v, dict):\n",
    "            if \"name\" in v and \"desc\" in v:\n",
    "                output[\"/\".join(new_path)] = {\"name\": v[\"name\"], \"desc\": v[\"desc\"]}\n",
    "            else:\n",
    "                output.update(_traverse_dict(v, new_path))\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_registry(fname: str = \"registry.yaml\"):\n",
    "    with open(fname) as f:\n",
    "        registry = yaml.safe_load(f)\n",
    "        return _traverse_dict(registry)\n",
    "\n",
    "\n",
    "api = wandb.Api()\n",
    "iapi = InternalApi()\n",
    "is_m1 = platform.machine() == \"arm64\" and platform.system() == \"Darwin\"\n",
    "\n",
    "entity = \"megatruong\"\n",
    "project = \"example-launch-jobs\"\n",
    "queue_name = \"andrew-cpu\"\n",
    "\n",
    "# git commit sha\n",
    "tag = \"5a0b4abc7b178cf28279c7f7a725d8c801792988\"\n",
    "\n",
    "# Point to env vars and creds to be mounted\n",
    "resource_args = {\n",
    "    \"local-container\": {\n",
    "        \"net\": \"host\",\n",
    "        \"env-file\": \"/Users/andrewtruong/.wandb_launch/env.list\",\n",
    "        \"volume\": [\n",
    "            \"/Users/andrewtruong/.aws:/home/andrewtruong/.aws:ro\",\n",
    "            \"/Users/andrewtruong/.aws:/root/.aws:ro\",\n",
    "        ],\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "kwargs_list = []\n",
    "registry = get_registry()\n",
    "for job_dir, metadata in registry.items():\n",
    "    *dir_parts, job_name = job_dir.split(\"/\")\n",
    "    img = f\"wandb/job_{job_name}:{tag}\"\n",
    "\n",
    "    if is_m1:\n",
    "        if job_name == \"sql_query\":\n",
    "            wandb.termwarn(\n",
    "                \"SQL Query Job cannot be run on M1 due to upstream connectorx issue.\"\n",
    "            )\n",
    "            continue\n",
    "    \n",
    "        elif job_name.startswith('gpu_'):\n",
    "            wandb.termwarn(f\"Job {job_name} requires an Nvidia GPU.\")\n",
    "            continue\n",
    "        \n",
    "        elif job_name == 'openai_evals':\n",
    "            wandb.termwarn(\"OpenAI Evals are bugged\")\n",
    "            continue\n",
    "    \n",
    "    # Create example runs for each job\n",
    "    configs_path = Path(\"jobs\", *dir_parts, job_name, \"configs\")\n",
    "    for path in configs_path.glob(\"*.yml\"):\n",
    "        with open(path) as f:\n",
    "            config = yaml.safe_load(f)\n",
    "\n",
    "        # launch_add.launch_add(\n",
    "        #     docker_image=img,\n",
    "        #     name=config.get(\"run_name\"),\n",
    "        #     config={\"overrides\": {\"run_config\": config.get(\"config\", {})}},\n",
    "        #     queue_name=queue_name,\n",
    "        #     entity=entity,\n",
    "        #     project=project,\n",
    "        # )\n",
    "\n",
    "        launch.run(\n",
    "            iapi,\n",
    "            docker_image=img,\n",
    "            name=config[\"run_name\"],\n",
    "            config={\"overrides\": {\"run_config\": config[\"config\"]}},\n",
    "            resource=\"local-container\",\n",
    "            resource_args=resource_args,\n",
    "            entity=entity,\n",
    "            project=project\n",
    "        )\n",
    "\n",
    "#         kwargs = {\n",
    "#             \"entity\": entity,\n",
    "#             \"project\": project,\n",
    "#             \"name\": config.get(\"run_name\"),\n",
    "#             \"docker_image\": img,\n",
    "#             \"config\": {\"overrides\": {\"run_config\": config.get(\"config\", {})}},\n",
    "#             \"resource\": \"local-container\",\n",
    "#             \"resource_args\": resource_args,\n",
    "#         }\n",
    "#         kwargs_list.append(kwargs)\n",
    "\n",
    "\n",
    "# with ThreadPoolExecutor(6) as exc:\n",
    "#     futures = {\n",
    "#         exc.submit(launch.run, iapi, **kwargs): i\n",
    "#         for i, kwargs in enumerate(kwargs_list)\n",
    "#     }\n",
    "#     for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "#         pass  # launch wont raise any errors; need to check later\n",
    "\n",
    "# 4. Cleanup sagemaker resources\n",
    "\n",
    "sagemaker = boto3.client(\"sagemaker\")\n",
    "response = sagemaker.list_endpoints()\n",
    "endpoints = response[\"Endpoints\"]\n",
    "\n",
    "for endpoint in endpoints:\n",
    "    try:\n",
    "        sagemaker.delete_endpoint(EndpointName=endpoint[\"EndpointName\"])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        wandb.termerror(f\"Problem deleting {endpoint['EndpointName']}\")\n",
    "    else:\n",
    "        wandb.termlog(f\"Successfully deleted {endpoint['EndpointName']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylaunch39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
