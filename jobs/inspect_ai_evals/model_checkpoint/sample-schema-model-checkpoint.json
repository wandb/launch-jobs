{
  "type": "object",
  "properties": {
    "tasks": {
      "type": "array",
      "title": "Evaluations",
      "description": "Select the tasks to run. For more information on the tasks, view [AISI Inspect Evals](https://inspect.aisi.org.uk/evals/).",
      "minItems": 1,
      "maxItems": 4,
      "placeholder": "Select evaluations",
      "items": {
        "type": "string",
        "enum": [
          "agentharm_benign",
          "agentic_misalignment",
          "agie_aqua_rat",
          "agie_logiqa_en",
          "agie_lsat_ar",
          "agie_lsat_lr",
          "agie_lsat_rc",
          "agie_math",
          "agie_sat_en",
          "agie_sat_en_without_passage",
          "agie_sat_math",
          "ahb",
          "aime2024",
          "aime2025",
          "air_bench",
          "arc_challenge",
          "arc_easy",
          "bbh",
          "bbeh",
          "bbeh_mini",
          "bbq",
          "bfcl",
          "bbh",
          "bbq",
          "bold",
          "boolq",
          "chembench",
          "coconot",
          "commonsense_qa",
          "cyse3_visual_prompt_injection",
          "docvqa",
          "drop",
          "gpqa_diamond",
          "gsm8k",
          "healthbench",
          "healthbench_consensus",
          "healthbench_hard",
          "hellaswag",
          "hle",
          "ifeval",
          "infinite_bench_code_debug",
          "infinite_bench_code_run",
          "infinite_bench_kv_retrieval",
          "infinite_bench_longbook_choice_eng",
          "infinite_bench_longdialogue_qa_eng",
          "infinite_bench_math_calc",
          "infinite_bench_math_find",
          "infinite_bench_number_string",
          "infinite_bench_passkey",
          "lab_bench_cloning_scenarios",
          "lab_bench_dbqa",
          "lab_bench_figqa",
          "lab_bench_litqa",
          "lab_bench_protocolqa",
          "lab_bench_seqqa",
          "lab_bench_suppqa",
          "lab_bench_tableqa",
          "lingoly",
          "lingoly_too",
          "make_me_pay",
          "mask",
          "math",
          "mathvista",
          "medqa",
          "mgsm",
          "mmiu",
          "mmlu_0_shot",
          "mmlu_5_shot",
          "mmlu_pro",
          "mmmu_multiple_choice",
          "mmmu_open",
          "musr",
          "niah",
          "onet_m6",
          "paws",
          "personality_BFI",
          "personality_TRAIT",
          "piqa",
          "pre_flight",
          "pubmedqa",
          "race_h",
          "sec_qa_v1",
          "sec_qa_v1_5_shot",
          "sec_qa_v2",
          "sec_qa_v2_5_shot",
          "sevenllm_mcq_en",
          "sevenllm_mcq_zh",
          "sevenllm_qa_en",
          "sevenllm_qa_zh",
          "simpleqa",
          "simpleqa_verified",
          "sosbench",
          "squad",
          "stereoset",
          "strong_reject",
          "sycophancy",
          "truthfulqa",
          "uccb",
          "vstar_bench_attribute_recognition",
          "vstar_bench_spatial_relationship_reasoning",
          "winogrande",
          "worldsense",
          "wmdp_bio",
          "wmdp_chem",
          "wmdp_cyber",
          "xstest"
        ]
      }
    },
    "limit": {
      "type": "integer",
      "title": "Sample limit",
      "description": "Maximum number of samples to evaluate for each task.",
      "placeholder": "E.g. 5",
      "exclusiveMinimum": 0
    },
    "artifact_path": {
      "type": "string",
      "title": "Model artifact",
      "description": "Path to the model artifact to use for the evaluation job.",
      "format": "artifact_path"
    },
    "create_leaderboard": {
      "type": "boolean",
      "title": "Leaderboard",
      "label": "Publish results to leaderboard",
      "description": "Evaluation results will be shown in a leaderboard. Once generated, leaderboards can be found in the Leaders page of your project."
    },
    "hf_token": {
      "type": "string",
      "title": "Hugging Face token",
      "description": "(Optional) Personal access token used to read gated datasets from Hugging Face.",
      "format": "secret"
    },
    "scorer_api_key": {
      "type": "string",
      "title": "Scorer API key",
      "description": "(Optional) Some evals use an OpenAI model as the default scorer.",
      "format": "secret"
    }
  }
}
