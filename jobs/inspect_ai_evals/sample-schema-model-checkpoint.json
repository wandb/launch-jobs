{
  "type": "object",
  "properties": {
    "tasks": {
      "type": "array",
      "title": "Tasks",
      "description": "Select the tasks to run. For more information on the tasks, see the docs at https://inspect.aisi.org.uk/evals/.",
      "minItems": 1,
      "maxItems": 4,
      "items": {
        "type": "string",
        "enum": [
          "inspect_evals/cybench",
          "inspect_evals/gdm_intercode_ctf",
          "inspect_evals/lab_bench_litqa",
          "inspect_evals/lab_bench_suppqa",
          "inspect_evals/lab_bench_figqa",
          "inspect_evals/lab_bench_tableqa",
          "inspect_evals/lab_bench_dbqa",
          "inspect_evals/lab_bench_protocolqa",
          "inspect_evals/lab_bench_seqqa",
          "inspect_evals/lab_bench_cloning_scenarios",
          "inspect_evals/hle",
          "inspect_evals/gpqa_diamond",
          "inspect_evals/aime2024",
          "inspect_evals/simpleqa",
          "inspect_evals/mmmu_multiple_choice",
          "inspect_evals/mmmu_open",
          "inspect_evals/infinite_bench_code_debug",
          "inspect_evals/infinite_bench_code_run",
          "inspect_evals/infinite_bench_kv_retrieval",
          "inspect_evals/infinite_bench_longbook_choice_eng",
          "inspect_evals/infinite_bench_longdialogue_qa_eng",
          "inspect_evals/infinite_bench_math_calc",
          "inspect_evals/infinite_bench_math_find",
          "inspect_evals/infinite_bench_number_string",
          "inspect_evals/infinite_bench_passkey",
          "inspect_evals/niah",
          "inspect_evals/mmlu_0_shot",
          "inspect_evals/mmlu_5_shot",
          "inspect_evals/mmlu_pro",
          "inspect_evals/strong_reject",
          "inspect_evals/sycophancy",
          "inspect_evals/bbq"
        ]
      }
    },
    "limit": {
      "type": "integer",
      "title": "Sample Limit",
      "description": "Maximum number of samples to evaluate for each task (e.g. 5, 10, 25, 50)"
    },
    "artifact_path": {
      "type": "string",
      "title": "Artifact Path",
      "description": "Path to the model artifact to use for the evaluation job",
      "format": "artifact_path"
    },
    "create_leaderboard": {
      "type": "boolean",
      "title": "Create a Leaderboard?",
      "description": "Choose to create a leaderboard from eval loggers. This will be updated with the results of the evaluation."
    },
    "hf_token": {
      "type": "string",
      "title": "Hugging Face Token",
      "description": "(Optional) Personal access token used to read gated datasets from Hugging Face.",
      "format": "secret"
    },
    "scorer_api_key": {
      "type": "string",
      "title": "Scorer API Key",
      "description": "(Optional) Some evals use an OpenAI model as the default scorer.",
      "format": "secret"
    }
  }
}
