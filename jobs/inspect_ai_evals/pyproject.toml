[project]
name = "inspect-ai-evals-wandb-job"
version = "0.1.0"
description = "Run a suite of off-the-shelf Inspect AI benchmarks against an LLM API and log the results to Weave"
readme = "README.md"
authors = [
    { name = "Bharat Ramanathan", email = "bharat.ramanathan@wandb.com" }
]
requires-python = ">=3.11"
dependencies = [
    "anthropic>=0.64.0",
    "black>=25.1.0",
    "huggingface-hub>=0.31.1",
    "datasets>=2.20.0",
    "inspect-ai>=0.3.69",
    "inspect-evals[bold,ifeval,sevenllm]",
    "inspect-wandb[weave]",
    "openai>=2.8.0",
    "pandas>=2.2.3",
    "pyarrow>=20.0.0",
    "pydantic==2.11.7",
    "pydantic-core==2.33.2",
    "pydantic-settings>=2.10.1",
    "sympy>=1.14.0",
    "wandb>=0.19.11",
    "weave>=0.51.59",
    "google-genai",
    "rouge",
    "instruction-following-eval",
]

[tool.uv.sources]
inspect-evals = { git = "https://github.com/UKGovernmentBEIS/inspect_evals" }
inspect-wandb = { git = "https://github.com/parambharat/inspect_wandb.git" }
instruction-following-eval = { git = "https://github.com/josejg/instruction_following_eval.git" }

[tool.inspect-wandb.weave]
enabled = true

[tool.inspect-wandb.models]
enabled = false
