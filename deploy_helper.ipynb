{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Page Loading Helper\n",
    "\n",
    "This notebook provides tooling to populate a jobs page from scratch using images from the [wandb dockerhub](https://hub.docker.com/u/wandb).\n",
    "\n",
    "Tooling includes:\n",
    "1. Create job from docker image\n",
    "2. Rename job to user-friendly name\n",
    "3. Add example runs that will auto-populate the run's `Clone from...` menu\n",
    "4. Delete the dummy run used to create the job initially\n",
    "\n",
    "Notes:\n",
    "1. This notebook uses a [special branch of the SDK](https://github.com/wandb/wandb/tree/andrew/helpers) with helpful GQL mutations added.  Please install that branch for now until it's merged into main.\n",
    "2. Jobs prefixed with `gpu_` require a GPU to run and are added to a GPU queue by default.  Please make sure you have a GPU  agent available to run these jobs, otherwise no runs will be populated.\n",
    "3. The `sql_query` job currently does not work on M1.  This is due to upstream issues with emulation and lack of linux/arm64 support for the `connectorx` package.  The job should still work on an `amd64` machine."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repo settings\n",
    "entity = 'megatruong'\n",
    "project = 'jobs32'\n",
    "\n",
    "# Queue settings\n",
    "cpu_queue_name = 'q2'\n",
    "gpu_queue_name = 'gpu'\n",
    "\n",
    "# Job/image settings\n",
    "tag = '134fcaf3d4b1499e69b426fad803b7e2cca85ab5'\n",
    "jobs_dir = 'jobs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.sdk.internal.internal_api import Api as InternalApi\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from wandb.sdk.launch import launch_add\n",
    "from functools import partial\n",
    "import docker\n",
    "import click\n",
    "\n",
    "\n",
    "api = wandb.Api()\n",
    "iapi = InternalApi()\n",
    "LOADER_STR = \"__loader-delete-me__\"\n",
    "\n",
    "\n",
    "def load_job(jobname, entity, project, queue_name, tag):\n",
    "    img = jobname2img(jobname, tag)\n",
    "    wandb.termlog(f\"Creating job: {entity}/{project}/{img}\")\n",
    "    create_job(img, entity, project)\n",
    "    \n",
    "    registry = get_registry()\n",
    "    ui_name, ui_desc = registry[jobname]['name'], registry[jobname]['desc']\n",
    "    artname = jobname2artname(jobname, tag)\n",
    "    artpath = artname2artpath(artname, entity, project, tag='latest')\n",
    "    wandb.termlog(f\"Renaming job to: {ui_name}\")\n",
    "    rename_job(artpath, ui_name, ui_desc)\n",
    "    \n",
    "    new_artpath = artname2artpath(ui_name, entity, project, tag='latest')\n",
    "    wandb.termlog(\"Adding new example runs...\")\n",
    "    add_example_runs(new_artpath, entity, project, queue_name)\n",
    "\n",
    "\n",
    "def create_job(img, entity, project):\n",
    "    \"\"\"\n",
    "    Create a job by running the docker image.\n",
    "    The run will show as failed because there is no config, but that's ok.  It will get deleted later.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # !docker run --env-file /Users/andrewtruong/.wandb_launch/env.list \\\n",
    "    #                 -e WANDB_ENTITY={entity} \\\n",
    "    #                 -e WANDB_PROJECT={project} \\\n",
    "    #                 -e WANDB_NAME={LOADER_STR} \\\n",
    "    #                 -e WANDB_DOCKER={img} \\\n",
    "    #                 --rm --net=host {img}\n",
    "                    \n",
    "    env = get_env(\"/Users/andrewtruong/.wandb_launch/env.list\")\n",
    "    env[\"WANDB_ENTITY\"] = entity\n",
    "    env[\"WANDB_PROJECT\"] = project\n",
    "    env[\"WANDB_NAME\"] = LOADER_STR\n",
    "    env[\"WANDB_DOCKER\"] = img\n",
    "\n",
    "    client = docker.from_env()\n",
    "    \n",
    "    emulation = True\n",
    "    if emulation:\n",
    "        container = client.containers.run(img, environment=env, detach=True, auto_remove=True, network_mode='host')\n",
    "    else:\n",
    "        container = client.containers.run(img, environment=env, detach=True, auto_remove=True, network_mode='host', platform='linux/amd64')    \n",
    "    \n",
    "    output = container.attach(stdout=True, stream=True, logs=True)\n",
    "    for line in output:\n",
    "        click.echo(line.decode('utf-8'), nl=False)\n",
    "\n",
    "                    \n",
    "                    \n",
    "def rename_job(job_path, new_name, new_desc):\n",
    "    \"\"\"\n",
    "    Rename the job from the default name to a pretty name and description we define in `registry.yaml`\n",
    "    \"\"\"\n",
    "    art = api.artifact(job_path)\n",
    "    asid = art._attrs['artifactSequence']['id']\n",
    "    \n",
    "    iapi.update_artifact_collection(asid, new_name, new_desc)\n",
    "\n",
    "\n",
    "def add_example_runs(job_art_path, entity, project, queue_name):\n",
    "    \"\"\"\n",
    "    Add example runs for the user to see and easily `Clone from...` in the UI.\n",
    "    \"\"\"\n",
    "    base_launcher = partial(launch_add.launch_add, job=job_art_path, project=project, entity=entity, queue_name=queue_name)\n",
    "    config_paths = Path(f'{jobs_dir}/{jobname}/configs').glob('*.yml')\n",
    "    \n",
    "    for p in config_paths:\n",
    "        with p.open() as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        base_launcher(config={\"overrides\": {\"run_config\": config['config']}}, name=config['run_name'])\n",
    "        \n",
    "\n",
    "def delete_loader_runs():\n",
    "    \"\"\"\n",
    "    Delete the unsightly \"loader\" run\n",
    "    \"\"\"\n",
    "    for run in api.runs(project):\n",
    "        if run.name == LOADER_STR:\n",
    "            run.delete()\n",
    "    \n",
    "\n",
    "def get_registry():\n",
    "    with open('registry.yaml') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def jobname2img(jobname, tag):\n",
    "    return f\"wandb/job_{jobname}:{tag}\"\n",
    "\n",
    "def get_jobnames(jobs_dir):\n",
    "    return [p.stem for p in Path(jobs_dir).glob('*')]\n",
    "\n",
    "def jobname2artname(jobname, tag):\n",
    "    return f\"job-wandb_job_{jobname}_{tag}\"\n",
    "\n",
    "def artname2artpath(artname, entity, project, tag=\"latest\"):\n",
    "    return f'{entity}/{project}/{artname}:{tag}'\n",
    "\n",
    "def get_env(envlist):\n",
    "    env = {}\n",
    "    with open(envlist) as f:\n",
    "        for line in f.read().splitlines():\n",
    "            k, v = line.split('=')\n",
    "            env[k] = v\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin up helper resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker run -p 3307:3306 -d sakiladb/mysql:latest\n",
    "# !docker build -t tritonserver-wandb jobs/deploy_to_nvidia_triton/server && \\\n",
    "#     docker run --rm --net=host -p 8000:8000 -v $HOME/.aws:/root/.aws:ro -d tritonserver-wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "is_m1 = platform.machine() == 'arm64' and platform.system() == \"Darwin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobnames = get_jobnames(jobs_dir)\n",
    "for jobname in jobnames:\n",
    "    if is_m1 and jobname == 'sql_query':\n",
    "        continue  # connectorx seems to cause issues with emulation on M1.\n",
    "    if jobname.startswith('gpu_'):\n",
    "        load_job(jobname, entity, project, gpu_queue_name, tag)\n",
    "    else:\n",
    "        load_job(jobname, entity, project, cpu_queue_name, tag)\n",
    "\n",
    "delete_loader_runs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete sagemaker endpoints that were spun up\n",
    "- You may have to run this manually because the jobs above need to actually run before the endpoints are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "\n",
    "response = sagemaker.list_endpoints()\n",
    "endpoints = response['Endpoints']\n",
    "\n",
    "for endpoint in endpoints:\n",
    "    try:\n",
    "        sagemaker.delete_endpoint(EndpointName=endpoint['EndpointName'])\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to see if any setup runs failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "for run in api.runs(project):\n",
    "    if run.state == 'failed':\n",
    "        for art in run.used_artifacts():\n",
    "            if art.type == 'job':\n",
    "                job = art.name\n",
    "                break\n",
    "        print(f\"{job}::{run.name} || {run}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
